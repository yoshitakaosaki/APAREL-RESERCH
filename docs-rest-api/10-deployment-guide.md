# ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¬ã‚¤ãƒ‰

## 1. æ¦‚è¦

REST APIã‚µãƒ¼ãƒãƒ¼ã‚’æœ¬ç•ªç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªã‚¬ã‚¤ãƒ‰ã§ã™ã€‚ç’°å¢ƒè¨­å®šã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ã€èªè¨¼ã‚µãƒ¼ãƒãƒ¼é€£æºã€CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãªã©ã€å®‰å®šã—ãŸé‹ç”¨ã«å¿…è¦ãªè¦ç´ ã‚’ç¶²ç¾…ã—ã¾ã™ã€‚

## 2. ç’°å¢ƒæ§‹æˆ

### 2.1 æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

```yaml
# æœ€å°è¦ä»¶
minimum:
  cpu: 2 cores
  memory: 4GB RAM
  storage: 20GB SSD
  
# æ¨å¥¨è¦ä»¶
recommended:
  cpu: 4 cores
  memory: 8GB RAM
  storage: 50GB SSD
  
# é«˜è² è·ç’°å¢ƒ
high_load:
  cpu: 8+ cores
  memory: 16GB+ RAM
  storage: 100GB+ SSD
  load_balancer: required
  cache_servers: 2+ Redis instances
```

### 2.2 ç’°å¢ƒå¤‰æ•°è¨­å®š

```bash
# .env.production

# Djangoè¨­å®š
SECRET_KEY=your-production-secret-key-here
DEBUG=False
ALLOWED_HOSTS=api.example.com,api2.example.com
ENVIRONMENT=production

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
DATABASE_URL=postgresql://apiuser:password@db.example.com:5432/apidb
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40
DB_POOL_TIMEOUT=30

# Redis
REDIS_URL=redis://:password@redis.example.com:6379/0
REDIS_POOL_SIZE=50
REDIS_SOCKET_TIMEOUT=5

# èªè¨¼ã‚µãƒ¼ãƒãƒ¼
AUTH_SERVER_URL=https://auth.example.com
JWKS_URL=https://auth.example.com/.well-known/jwks.json
OAUTH_ISSUER=https://auth.example.com
OAUTH_AUDIENCE=api.example.com

# CORS
CORS_ALLOWED_ORIGINS=https://app.example.com,https://admin.example.com

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
SESSION_COOKIE_SECURE=True
CSRF_COOKIE_SECURE=True
SECURE_SSL_REDIRECT=True
SECURE_HSTS_SECONDS=31536000
SECURE_HSTS_INCLUDE_SUBDOMAINS=True
SECURE_HSTS_PRELOAD=True

# ãƒ­ã‚°
LOG_LEVEL=INFO
SENTRY_DSN=https://xxxxx@sentry.io/project-id

# ãƒ¡ãƒ¼ãƒ«
EMAIL_HOST=smtp.example.com
EMAIL_PORT=587
EMAIL_USE_TLS=True
EMAIL_HOST_USER=notifications@example.com
EMAIL_HOST_PASSWORD=email-password

# ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
AWS_ACCESS_KEY_ID=your-aws-key
AWS_SECRET_ACCESS_KEY=your-aws-secret
AWS_STORAGE_BUCKET_NAME=api-storage-bucket
AWS_S3_REGION_NAME=ap-northeast-1

# ç›£è¦–
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/xxx/xxx/xxx
PAGERDUTY_INTEGRATION_KEY=your-pagerduty-key
PROMETHEUS_METRICS_ENABLED=True
```

## 3. Dockerè¨­å®š

### 3.1 æœ¬ç•ªç”¨Dockerfile

```dockerfile
# Dockerfile.production
FROM python:3.10-slim as builder

# ãƒ“ãƒ«ãƒ‰å¼•æ•°
ARG DEBIAN_FRONTEND=noninteractive

# ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    libssl-dev \
    libffi-dev \
    git \
    && rm -rf /var/lib/apt/lists/*

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
WORKDIR /app

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡åŒ–ï¼‰
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰ï¼ˆæœ€çµ‚ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼‰
FROM python:3.10-slim

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
RUN apt-get update && apt-get upgrade -y \
    && apt-get install -y \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# érootãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
RUN useradd -m -u 1000 -s /bin/bash appuser

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
WORKDIR /app

# ãƒ“ãƒ«ãƒ€ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¸ã‹ã‚‰ä¾å­˜é–¢ä¿‚ã‚³ãƒ”ãƒ¼
COPY --from=builder /root/.local /home/appuser/.local

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼
COPY --chown=appuser:appuser . .

# ãƒ‘ã‚¹è¨­å®š
ENV PATH=/home/appuser/.local/bin:$PATH

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«åé›†
RUN python manage.py collectstatic --noinput

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ‡ã‚Šæ›¿ãˆ
USER appuser

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/api/v1/health || exit 1

# ãƒãƒ¼ãƒˆ
EXPOSE 8000

# èµ·å‹•ã‚³ãƒãƒ³ãƒ‰
CMD ["gunicorn", "api_server.wsgi:application", \
     "--bind", "0.0.0.0:8000", \
     "--workers", "4", \
     "--threads", "2", \
     "--worker-class", "sync", \
     "--worker-tmp-dir", "/dev/shm", \
     "--access-logfile", "-", \
     "--error-logfile", "-", \
     "--log-level", "info", \
     "--timeout", "60", \
     "--graceful-timeout", "30", \
     "--max-requests", "1000", \
     "--max-requests-jitter", "100"]
```

### 3.2 Docker Composeè¨­å®š

```yaml
# docker-compose.production.yml
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile.production
    image: api-server:latest
    container_name: api_server
    restart: unless-stopped
    environment:
      - DJANGO_SETTINGS_MODULE=api_server.settings.production
    env_file:
      - .env.production
    ports:
      - "8000:8000"
    volumes:
      - static_volume:/app/staticfiles
      - media_volume:/app/media
      - logs_volume:/app/logs
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - api_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "10"

  db:
    image: postgres:15-alpine
    container_name: api_db
    restart: unless-stopped
    environment:
      POSTGRES_DB: apidb
      POSTGRES_USER: apiuser
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - api_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U apiuser -d apidb"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: api_redis
    restart: unless-stopped
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - redis_data:/data
    networks:
      - api_network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  nginx:
    image: nginx:alpine
    container_name: api_nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/conf.d:/etc/nginx/conf.d
      - static_volume:/var/www/static
      - media_volume:/var/www/media
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - api
    networks:
      - api_network

volumes:
  postgres_data:
  redis_data:
  static_volume:
  media_volume:
  logs_volume:

networks:
  api_network:
    driver: bridge
```

### 3.3 Nginxè¨­å®š

```nginx
# nginx/conf.d/api.conf
upstream api_backend {
    least_conn;
    server api:8000 max_fails=3 fail_timeout=30s;
    keepalive 32;
}

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¾ãƒ¼ãƒ³
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=auth_limit:10m rate=5r/s;

server {
    listen 80;
    server_name api.example.com;
    
    # HTTPSã¸ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name api.example.com;
    
    # SSLè¨­å®š
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" always;
    add_header X-Frame-Options "DENY" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    
    # gzipåœ§ç¸®
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript application/json application/javascript application/xml+rss;
    
    # é™çš„ãƒ•ã‚¡ã‚¤ãƒ«
    location /static/ {
        alias /var/www/static/;
        expires 30d;
        add_header Cache-Control "public, immutable";
    }
    
    location /media/ {
        alias /var/www/media/;
        expires 7d;
        add_header Cache-Control "public";
    }
    
    # API
    location /api/ {
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        limit_req zone=api_limit burst=20 nodelay;
        
        proxy_pass http://api_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        proxy_connect_timeout 30s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
        proxy_busy_buffers_size 8k;
        
        # ã‚­ãƒ¼ãƒ—ã‚¢ãƒ©ã‚¤ãƒ–
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
    
    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ãªã—ï¼‰
    location = /api/v1/health {
        proxy_pass http://api_backend;
        proxy_set_header Host $host;
        access_log off;
    }
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆå†…éƒ¨ã‚¢ã‚¯ã‚»ã‚¹ã®ã¿ï¼‰
    location /metrics {
        allow 10.0.0.0/8;
        deny all;
        proxy_pass http://api_backend;
    }
}
```

## 4. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ã‚¹

### 4.1 ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# deploy.sh

set -e

# è¨­å®š
ENVIRONMENT=${1:-production}
VERSION=${2:-latest}
DOCKER_REGISTRY="your-registry.com"
IMAGE_NAME="api-server"

echo "ğŸš€ Starting deployment for $ENVIRONMENT environment..."

# 1. ç’°å¢ƒãƒã‚§ãƒƒã‚¯
if [ ! -f ".env.$ENVIRONMENT" ]; then
    echo "âŒ Environment file .env.$ENVIRONMENT not found!"
    exit 1
fi

# 2. ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰
echo "ğŸ”¨ Building Docker image..."
docker build -f Dockerfile.production -t $IMAGE_NAME:$VERSION .

# 3. ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚¿ã‚°ä»˜ã‘ã¨ãƒ—ãƒƒã‚·ãƒ¥
echo "ğŸ“¤ Pushing image to registry..."
docker tag $IMAGE_NAME:$VERSION $DOCKER_REGISTRY/$IMAGE_NAME:$VERSION
docker tag $IMAGE_NAME:$VERSION $DOCKER_REGISTRY/$IMAGE_NAME:latest
docker push $DOCKER_REGISTRY/$IMAGE_NAME:$VERSION
docker push $DOCKER_REGISTRY/$IMAGE_NAME:latest

# 4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
echo "ğŸ—„ï¸ Running database migrations..."
docker-compose -f docker-compose.$ENVIRONMENT.yml run --rm api python manage.py migrate

# 5. é™çš„ãƒ•ã‚¡ã‚¤ãƒ«åé›†
echo "ğŸ“ Collecting static files..."
docker-compose -f docker-compose.$ENVIRONMENT.yml run --rm api python manage.py collectstatic --noinput

# 6. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "ğŸ¥ Running pre-deployment health check..."
docker-compose -f docker-compose.$ENVIRONMENT.yml run --rm api python manage.py check --deploy

# 7. ãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ
echo "ğŸš¢ Deploying new version..."
docker-compose -f docker-compose.$ENVIRONMENT.yml pull
docker-compose -f docker-compose.$ENVIRONMENT.yml up -d --remove-orphans

# 8. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å¾…æ©Ÿ
echo "â³ Waiting for health check..."
sleep 10

# 9. ãƒ‡ãƒ—ãƒ­ã‚¤ç¢ºèª
echo "âœ… Verifying deployment..."
curl -f http://localhost/api/v1/health || {
    echo "âŒ Health check failed!"
    docker-compose -f docker-compose.$ENVIRONMENT.yml logs --tail=100
    exit 1
}

echo "âœ¨ Deployment completed successfully!"
```

### 4.2 ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ

```bash
#!/bin/bash
# rolling-update.sh

set -e

# è¨­å®š
REPLICAS=3
SERVICE_NAME="api"
HEALTH_CHECK_URL="http://localhost/api/v1/health"
GRACE_PERIOD=30

echo "ğŸ”„ Starting rolling update..."

# ç¾åœ¨ã®å®Ÿè¡Œä¸­ã‚³ãƒ³ãƒ†ãƒŠå–å¾—
OLD_CONTAINERS=$(docker ps -q -f name=${SERVICE_NAME}_)

# æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒŠã‚’é †æ¬¡èµ·å‹•
for i in $(seq 1 $REPLICAS); do
    echo "Starting new container $i/$REPLICAS..."
    
    # æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•
    docker-compose up -d --scale $SERVICE_NAME=$((i))
    
    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å¾…æ©Ÿ
    sleep 10
    
    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    if ! curl -f $HEALTH_CHECK_URL; then
        echo "âŒ Health check failed for new container!"
        docker-compose down
        exit 1
    fi
    
    echo "âœ… Container $i is healthy"
done

# å¤ã„ã‚³ãƒ³ãƒ†ãƒŠã‚’é †æ¬¡åœæ­¢
echo "Removing old containers..."
for container in $OLD_CONTAINERS; do
    echo "Gracefully stopping container $container..."
    docker stop --time=$GRACE_PERIOD $container
    docker rm $container
    sleep 5
done

echo "âœ¨ Rolling update completed!"
```

## 5. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥

### 5.1 æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¨­å®š

```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  labels:
    app: api-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      containers:
      - name: api
        image: your-registry.com/api-server:latest
        ports:
        - containerPort: 8000
        env:
        - name: DJANGO_SETTINGS_MODULE
          value: "api_server.settings.production"
        envFrom:
        - secretRef:
            name: api-secrets
        - configMapRef:
            name: api-config
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /api/v1/health/liveness
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/v1/health/readiness
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: static-files
          mountPath: /app/staticfiles
        - name: media-files
          mountPath: /app/media
      volumes:
      - name: static-files
        persistentVolumeClaim:
          claimName: static-pvc
      - name: media-files
        persistentVolumeClaim:
          claimName: media-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: api-server
spec:
  selector:
    app: api-server
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-server-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 60
```

### 5.2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–

```python
# api/caching/strategies.py
from django.core.cache import cache
from django.conf import settings
from functools import wraps
import hashlib
import json

class CacheStrategy:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def cache_key_generator(prefix: str, *args, **kwargs):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        key_data = {
            'args': args,
            'kwargs': kwargs
        }
        key_hash = hashlib.md5(
            json.dumps(key_data, sort_keys=True).encode()
        ).hexdigest()
        return f"{prefix}:{key_hash}"
    
    @staticmethod
    def cache_response(
        timeout: int = 300,
        key_prefix: str = None,
        vary_on: list = None
    ):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
        def decorator(func):
            @wraps(func)
            def wrapper(request, *args, **kwargs):
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
                cache_key_parts = [key_prefix or func.__name__]
                
                # vary_on ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‡¦ç†
                if vary_on:
                    for param in vary_on:
                        if param == 'user':
                            cache_key_parts.append(
                                getattr(request, 'user_id', 'anonymous')
                            )
                        elif param == 'query':
                            cache_key_parts.append(
                                request.GET.urlencode()
                            )
                
                cache_key = CacheStrategy.cache_key_generator(
                    ':'.join(cache_key_parts),
                    *args,
                    **kwargs
                )
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
                cached_response = cache.get(cache_key)
                if cached_response is not None:
                    return cached_response
                
                # å®Ÿè¡Œã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                response = func(request, *args, **kwargs)
                
                # æˆåŠŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                if response.status_code == 200:
                    cache.set(cache_key, response, timeout)
                
                return response
            
            return wrapper
        return decorator

# ä½¿ç”¨ä¾‹
@api_view(['GET'])
@CacheStrategy.cache_response(
    timeout=3600,  # 1æ™‚é–“
    key_prefix='user_profile',
    vary_on=['user']
)
def get_user_profile(request):
    # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—å‡¦ç†
    pass
```

## 6. èªè¨¼ã‚µãƒ¼ãƒãƒ¼é€£æº

### 6.1 é«˜å¯ç”¨æ€§JWKSå–å¾—

```python
# api/authentication/ha_jwks.py
import requests
from typing import Dict, List, Optional
import time
import random
from django.conf import settings
from django.core.cache import cache
import logging

logger = logging.getLogger(__name__)

class HighAvailabilityJWKS:
    """é«˜å¯ç”¨æ€§JWKSå–å¾—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # è¤‡æ•°ã®èªè¨¼ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        self.auth_servers = settings.AUTH_SERVERS  # ãƒªã‚¹ãƒˆå½¢å¼
        self.timeout = 5
        self.retry_count = 3
        self.cache_timeout = 3600  # 1æ™‚é–“
    
    def get_jwks(self) -> Optional[Dict]:
        """JWKSã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰"""
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cached_jwks = cache.get('jwks:data')
        if cached_jwks:
            return cached_jwks
        
        # è¤‡æ•°ã®èªè¨¼ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        servers = self.auth_servers.copy()
        random.shuffle(servers)  # ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°
        
        for server in servers:
            jwks = self._fetch_from_server(server)
            if jwks:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                cache.set('jwks:data', jwks, self.cache_timeout)
                return jwks
        
        # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã€å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
        stale_jwks = cache.get('jwks:data:stale')
        if stale_jwks:
            logger.warning("Using stale JWKS cache")
            return stale_jwks
        
        raise Exception("Failed to fetch JWKS from all servers")
    
    def _fetch_from_server(self, server_url: str) -> Optional[Dict]:
        """ç‰¹å®šã®ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰JWKSå–å¾—"""
        jwks_url = f"{server_url}/.well-known/jwks.json"
        
        for attempt in range(self.retry_count):
            try:
                response = requests.get(
                    jwks_url,
                    timeout=self.timeout,
                    headers={
                        'User-Agent': 'API-Server/1.0',
                        'Accept': 'application/json'
                    }
                )
                
                if response.status_code == 200:
                    jwks = response.json()
                    
                    # Staleã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚æ›´æ–°
                    cache.set('jwks:data:stale', jwks, None)  # æ°¸ç¶šåŒ–
                    
                    return jwks
                    
            except requests.RequestException as e:
                logger.warning(
                    f"Failed to fetch JWKS from {server_url} "
                    f"(attempt {attempt + 1}/{self.retry_count}): {str(e)}"
                )
                
                # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                if attempt < self.retry_count - 1:
                    time.sleep(2 ** attempt)
        
        return None
    
    def preload_jwks(self):
        """èµ·å‹•æ™‚ã«JWKSã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰"""
        try:
            self.get_jwks()
            logger.info("JWKS preloaded successfully")
        except Exception as e:
            logger.error(f"Failed to preload JWKS: {str(e)}")
```

### 6.2 èªè¨¼ã‚µãƒ¼ãƒãƒ¼ç›£è¦–

```python
# api/monitoring/auth_server_monitor.py
from django.core.management.base import BaseCommand
import requests
import time
from datetime import datetime
from api.monitoring.alerts import AlertManager

class AuthServerMonitor:
    """èªè¨¼ã‚µãƒ¼ãƒãƒ¼ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.auth_servers = settings.AUTH_SERVERS
        self.alert_manager = AlertManager()
        self.check_interval = 60  # 60ç§’ã”ã¨
    
    def run(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ"""
        while True:
            for server in self.auth_servers:
                self.check_server(server)
            
            time.sleep(self.check_interval)
    
    def check_server(self, server_url: str):
        """ã‚µãƒ¼ãƒãƒ¼ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        health_url = f"{server_url}/health"
        
        try:
            start_time = time.time()
            response = requests.get(health_url, timeout=10)
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                auth_server_health_gauge.labels(
                    server=server_url
                ).set(1)
                
                auth_server_response_time.labels(
                    server=server_url
                ).observe(response_time)
                
            else:
                # ç•°å¸¸æ¤œçŸ¥
                self.handle_unhealthy_server(
                    server_url,
                    f"HTTP {response.status_code}"
                )
                
        except Exception as e:
            self.handle_unhealthy_server(server_url, str(e))
    
    def handle_unhealthy_server(self, server_url: str, error: str):
        """ç•°å¸¸ã‚µãƒ¼ãƒãƒ¼ã®å‡¦ç†"""
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        auth_server_health_gauge.labels(server=server_url).set(0)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
        self.alert_manager.send_slack_alert(
            title="Auth Server Unhealthy",
            message=f"Server {server_url} is unhealthy: {error}",
            severity='warning'
        )
        
        # ã‚µãƒ¼ãƒãƒ¼ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        cache.set(
            f"auth_server:disabled:{server_url}",
            True,
            300  # 5åˆ†é–“ç„¡åŠ¹åŒ–
        )
```

## 7. CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 7.1 GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  DOCKER_REGISTRY: your-registry.com
  IMAGE_NAME: api-server

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run tests
        run: |
          pytest --cov=api --cov-fail-under=80
      
      - name: Security scan
        run: |
          pip install safety bandit
          safety check
          bandit -r api/

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Log in to registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./Dockerfile.production
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}:latest
            ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment: production
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to production
        env:
          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }}
          DEPLOY_HOST: ${{ secrets.DEPLOY_HOST }}
          DEPLOY_USER: ${{ secrets.DEPLOY_USER }}
        run: |
          echo "$DEPLOY_KEY" > deploy_key
          chmod 600 deploy_key
          
          ssh -i deploy_key -o StrictHostKeyChecking=no \
            $DEPLOY_USER@$DEPLOY_HOST \
            "cd /opt/api-server && \
             git pull && \
             docker-compose pull && \
             docker-compose up -d --remove-orphans && \
             docker system prune -f"
      
      - name: Health check
        run: |
          sleep 30
          curl -f https://api.example.com/api/v1/health || exit 1
      
      - name: Notify Slack
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Production deployment ${{ job.status }}'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### 7.2 è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

```bash
#!/bin/bash
# rollback.sh

set -e

# è¨­å®š
PREVIOUS_VERSION=${1:-"previous"}
HEALTH_CHECK_URL="https://api.example.com/api/v1/health"
MAX_RETRIES=5

echo "ğŸ”™ Starting rollback to version: $PREVIOUS_VERSION"

# 1. ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
CURRENT_VERSION=$(docker ps --format "table {{.Image}}" | grep api-server | head -1)
echo "Current version: $CURRENT_VERSION"

# 2. ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
docker-compose down
docker-compose pull api-server:$PREVIOUS_VERSION
docker-compose up -d

# 3. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "Waiting for service to be healthy..."
for i in $(seq 1 $MAX_RETRIES); do
    sleep 10
    if curl -f $HEALTH_CHECK_URL; then
        echo "âœ… Service is healthy!"
        break
    fi
    
    if [ $i -eq $MAX_RETRIES ]; then
        echo "âŒ Health check failed after $MAX_RETRIES attempts"
        exit 1
    fi
done

# 4. æ¤œè¨¼
echo "Verifying rollback..."
NEW_VERSION=$(docker ps --format "table {{.Image}}" | grep api-server | head -1)
echo "Rolled back to: $NEW_VERSION"

# 5. ã‚¢ãƒ©ãƒ¼ãƒˆ
curl -X POST $SLACK_WEBHOOK_URL \
    -H 'Content-Type: application/json' \
    -d "{
        \"text\": \"ğŸ”™ Rollback completed\",
        \"attachments\": [{
            \"color\": \"warning\",
            \"fields\": [
                {\"title\": \"From Version\", \"value\": \"$CURRENT_VERSION\", \"short\": true},
                {\"title\": \"To Version\", \"value\": \"$NEW_VERSION\", \"short\": true}
            ]
        }]
    }"

echo "âœ¨ Rollback completed successfully!"
```

## 8. ç›£è¦–ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

### 8.1 ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç›£è¦–è¨­å®š

```yaml
# monitoring/prometheus-production.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    environment: 'production'
    service: 'api-server'

rule_files:
  - '/etc/prometheus/rules/*.yml'

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

scrape_configs:
  - job_name: 'api-server'
    static_configs:
      - targets: 
          - api1.example.com:8000
          - api2.example.com:8000
          - api3.example.com:8000
    metrics_path: '/metrics'
    
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:9113']
    
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
```

### 8.2 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

```bash
#!/bin/bash
# backup.sh

set -e

# è¨­å®š
BACKUP_DIR="/backup/api-server"
S3_BUCKET="s3://your-backup-bucket/api-server"
RETENTION_DAYS=30

# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

echo "ğŸ”’ Starting backup process..."

# 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "Backing up database..."
docker exec api_db pg_dump -U apiuser apidb | gzip > $BACKUP_DIR/db_$TIMESTAMP.sql.gz

# 2. ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "Backing up media files..."
tar -czf $BACKUP_DIR/media_$TIMESTAMP.tar.gz -C /var/www media/

# 3. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "Backing up configuration..."
tar -czf $BACKUP_DIR/config_$TIMESTAMP.tar.gz \
    .env.production \
    docker-compose.production.yml \
    nginx/

# 4. S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
echo "Uploading to S3..."
aws s3 cp $BACKUP_DIR/db_$TIMESTAMP.sql.gz $S3_BUCKET/db/
aws s3 cp $BACKUP_DIR/media_$TIMESTAMP.tar.gz $S3_BUCKET/media/
aws s3 cp $BACKUP_DIR/config_$TIMESTAMP.tar.gz $S3_BUCKET/config/

# 5. å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤
echo "Cleaning up old backups..."
find $BACKUP_DIR -type f -mtime +$RETENTION_DAYS -delete
aws s3 ls $S3_BUCKET/ --recursive | \
    awk -v date="$(date -d "-$RETENTION_DAYS days" +%Y-%m-%d)" '$1 < date {print $4}' | \
    xargs -I {} aws s3 rm $S3_BUCKET/{}

echo "âœ… Backup completed successfully!"
```

## ã¾ã¨ã‚

ã“ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¬ã‚¤ãƒ‰ã«å¾“ã†ã“ã¨ã§ã€REST APIã‚µãƒ¼ãƒãƒ¼ã‚’å®‰å…¨ã‹ã¤åŠ¹ç‡çš„ã«æœ¬ç•ªç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ï¼š

1. **ç’°å¢ƒæ§‹æˆ**: æœ¬ç•ªç’°å¢ƒã«é©ã—ãŸè¨­å®šã¨æœ€é©åŒ–
2. **ã‚³ãƒ³ãƒ†ãƒŠåŒ–**: Dockerã«ã‚ˆã‚‹ä¸€è²«ã—ãŸå®Ÿè¡Œç’°å¢ƒ
3. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: è² è·ã«å¿œã˜ãŸè‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
4. **é«˜å¯ç”¨æ€§**: èªè¨¼ã‚µãƒ¼ãƒãƒ¼ã¨ã®å†—é•·æ¥ç¶š
5. **è‡ªå‹•åŒ–**: CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚‹ç¶™ç¶šçš„ãƒ‡ãƒ—ãƒ­ã‚¤
6. **ç›£è¦–**: åŒ…æ‹¬çš„ãªç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆä½“åˆ¶
7. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: å®šæœŸçš„ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ãƒªã‚«ãƒãƒªè¨ˆç”»